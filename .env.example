# === Copy this file to .env and set real values ===

# Upstream base URL (choose ONE):
# - GitHub Models API (recommended)
OPENAI_BASE_URL=https://models.github.ai/inference
# - Or OpenAI API
# OPENAI_BASE_URL=https://api.openai.com/v1

# Auth mode for the proxy: env | request | prefer-request
# - env: use server-side token from .env only (client key ignored)
# - request: use only client-provided token (Authorization / X-OpenAI-Api-Key)
# - prefer-request: use client token if present, otherwise fallback to .env (default)
PROXY_AUTH_MODE=env

# Optional server access key (keeps your real token private)
# If set, clients must send this key to access /v1 and /api routes.
# Accepted as header:  X-Proxy-Server-Key: <key>
# Or (when PROXY_AUTH_MODE=env) as: Authorization: Bearer <key>
PROXY_SERVER_KEY=

# Provide ONE token depending on OPENAI_BASE_URL
# For GitHub Models API:
GITHUB_TOKEN=

# For OpenAI API:
# OPENAI_API_KEY=

# Port to run the proxy
PORT=3010

# Optional: suppress warning when key is missing
# SUPPRESS_KEY_WARN=1

# Strict OpenAI-compatible API only (disable UI and custom endpoints)
STRICT_OPENAI_API=true

# Disallow clients to override upstream base URL via headers (default: disabled)
# Set to true to allow X-OpenAI-Base-Url header when not in strict mode.
ALLOW_BASE_URL_OVERRIDE=false
